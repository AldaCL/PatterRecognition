{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Funciones de Pérdida en Machine Learning**\n",
    "\n",
    "| **Tipo de Problema**  | **Función de Pérdida**          | **Fórmula**                                                                 | **Razón de Uso** |\n",
    "|----------------------|------------------------------|---------------------------------------------------------------------------|------------------|\n",
    "| **Regresión**       | **Error Cuadrático Medio (MSE)** | $$ J(W) = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i)^2 $$ | Penaliza más los errores grandes, útil cuando los errores grandes son costosos. |\n",
    "| **Regresión**       | **Error Absoluto Medio (MAE)** | $$ J(W) = \\frac{1}{m} \\sum_{i=1}^{m} \\| \\hat{y}_i - y_i \\| $$ | Es más robusto ante valores atípicos, ya que penaliza menos los errores grandes. |\n",
    "| **Regresión**       | **Error Huber** | $$ J(W) = \\sum_{i=1}^{m} L_{\\delta}(\\hat{y}_i - y_i) $$ con $$ L_{\\delta}(a) = \\begin{cases} \\frac{1}{2} a^2 & \\text{si } \\|a\\| \\leq \\delta \\\\ \\delta (\\|a\\| - \\frac{1}{2} \\delta) & \\text{si } \\|a\\| > \\delta \\end{cases} $$ | Combina MSE y MAE para ser robusto a valores atípicos sin perder precisión en errores pequeños. |\n",
    "| **Regresión**       | **Log-Cosh** | $$ J(W) = \\sum_{i=1}^{m} \\log(\\cosh(\\hat{y}_i - y_i)) $$ | Similar a Huber, pero más suave, lo que mejora la estabilidad numérica. |\n",
    "| **Clasificación Binaria** | **Entropía Cruzada (Log Loss)** | $$ J(W) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] $$ | Mide la diferencia entre distribuciones de probabilidad, útil para problemas binarios. |\n",
    "| **Clasificación Multiclase** | **Entropía Cruzada Categórica** | $$ J(W) = - \\sum_{i=1}^{m} \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{y}_{i,c}) $$ | Extensión de la entropía cruzada para múltiples clases. Se usa con softmax. |\n",
    "| **Clasificación Multietiqueta** | **Entropía Cruzada con Sigmoide** | $$ J(W) = - \\sum_{i=1}^{m} \\sum_{c=1}^{C} \\left[ y_{i,c} \\log(\\hat{y}_{i,c}) + (1 - y_{i,c}) \\log(1 - \\hat{y}_{i,c}) \\right] $$ | Se usa en problemas donde cada instancia puede pertenecer a múltiples clases simultáneamente. |\n",
    "| **Clasificación** | **Hinge Loss (SVM Loss)** | $$ J(W) = \\sum_{i=1}^{m} \\max(0, 1 - y_i \\hat{y}_i) $$ | Se usa en SVMs para maximizar el margen de separación entre clases. |\n",
    "| **Clasificación** | **Kullback-Leibler Divergence (KL Loss)** | $$ J(W) = \\sum_{i} P(x_i) \\log \\frac{P(x_i)}{Q(x_i)} $$ | Mide la diferencia entre dos distribuciones de probabilidad, útil en modelos probabilísticos. |\n",
    "\n",
    "---\n",
    "\n",
    "## **Notas:**\n",
    "- **MSE** es más sensible a valores atípicos porque eleva al cuadrado el error.  \n",
    "- **MAE** es más robusto ante valores atípicos pero menos suave para optimización.  \n",
    "- **Huber y Log-Cosh** combinan MSE y MAE, equilibrando precisión y robustez.  \n",
    "- **La entropía cruzada** es la opción estándar para clasificación porque mide la diferencia entre la distribución de salida y la esperada.  \n",
    "- **Hinge Loss** se usa en SVMs para maximizar el margen entre clases.  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

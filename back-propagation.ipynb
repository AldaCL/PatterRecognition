{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Backpropagation y Algoritmo de Descenso de Gradiente**\n",
    "\n",
    "## **驴Qu茅 es el Backpropagation?**\n",
    "El **Backpropagation** (retropropagaci贸n) es un algoritmo de optimizaci贸n utilizado en redes neuronales para **ajustar los pesos** y minimizar el error de predicci贸n. Funciona propagando el error desde la capa de salida hacia las capas anteriores, ajustando los pesos mediante **descenso del gradiente**.\n",
    "\n",
    "Se basa en dos fases:\n",
    "1. **Forward Pass (Propagaci贸n hacia adelante):** La red genera una predicci贸n.\n",
    "2. **Backward Pass (Retropropagaci贸n del error):** Calculamos el error y ajustamos los pesos.\n",
    "\n",
    "---\n",
    "\n",
    "## **驴Qu茅 es el Descenso de Gradiente?**\n",
    "El **Descenso de Gradiente** es un algoritmo que encuentra los valores 贸ptimos de los pesos minimizando una funci贸n de costo $( J(W))$. Se basa en calcular la derivada (gradiente) de la funci贸n de costo con respecto a los pesos y actualizar los valores en la direcci贸n opuesta al gradiente.\n",
    "\n",
    "### **F贸rmula general del Descenso de Gradiente**\n",
    "$$\n",
    "W := W - \\alpha \\frac{\\partial J}{\\partial W}\n",
    "$$\n",
    "Donde:\n",
    "- $ ( W ) $ son los pesos del modelo.\n",
    "- $ ( \\alpha )$ es la tasa de aprendizaje (learning rate).\n",
    "- $ ( \\frac{\\partial J}{\\partial W} ) $ es el gradiente de la funci贸n de costo respecto a $( W )$ .\n",
    "\n",
    "---\n",
    "\n",
    "## **Matem谩tica del Backpropagation**\n",
    "### **Paso 1: Forward Pass**\n",
    "Dado un conjunto de datos de entrada $( X )$ y pesos $( W )$, la salida de una neurona en la capa oculta es:\n",
    "\n",
    "$$\n",
    "Z = W X + b\n",
    "$$\n",
    "\n",
    "Aplicamos la **funci贸n de activaci贸n** $( f(Z) )$ para obtener la salida de la neurona:\n",
    "\n",
    "$$\n",
    "A = f(Z)\n",
    "$$\n",
    "\n",
    "En la capa de salida, se obtiene una predicci贸n $( \\hat{y} )$:\n",
    "\n",
    "$$\n",
    "\\hat{y} = f(W^{(2)} A + b^{(2)})\n",
    "$$\n",
    "\n",
    "### **Paso 2: C谩lculo del Error**\n",
    "Definimos una funci贸n de p茅rdida $( J(W) )$, por ejemplo, el error cuadr谩tico medio (MSE) en regresi贸n:\n",
    "\n",
    "$$\n",
    "J(W) = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i)^2\n",
    "$$\n",
    "\n",
    "o la entrop铆a cruzada en clasificaci贸n:\n",
    "\n",
    "$$\n",
    "J(W) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "### **Paso 3: Backpropagation - C谩lculo del Gradiente**\n",
    "Usamos la **regla de la cadena** para calcular la derivada de la p茅rdida respecto a cada peso:\n",
    "\n",
    "1. **Error en la salida:**\n",
    "   $$\n",
    "   \\delta^{(2)} = \\frac{\\partial J}{\\partial \\hat{y}} \\cdot f'(Z^{(2)})\n",
    "   $$\n",
    "\n",
    "2. **Error en la capa oculta:**\n",
    "   $$\n",
    "   \\delta^{(1)} = (\\delta^{(2)} W^{(2)}) \\cdot f'(Z^{(1)})\n",
    "   $$\n",
    "\n",
    "3. **Gradiente de los pesos:**\n",
    "   $$\n",
    "   \\frac{\\partial J}{\\partial W^{(2)}} = A^{(1)} \\delta^{(2)}\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial J}{\\partial W^{(1)}} = X \\delta^{(1)}\n",
    "   $$\n",
    "\n",
    "### **Paso 4: Actualizaci贸n de Pesos**\n",
    "$$\n",
    "W^{(l)} := W^{(l)} - \\alpha \\frac{\\partial J}{\\partial W^{(l)}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **Resumen del Algoritmo de Backpropagation**\n",
    "1. **Propagaci贸n hacia adelante:** Calculamos la salida de la red.\n",
    "2. **C谩lculo del error:** Evaluamos la diferencia entre la salida esperada y la salida real.\n",
    "3. **Retropropagaci贸n del error:** Calculamos el gradiente en cada capa usando la regla de la cadena.\n",
    "4. **Actualizaci贸n de pesos:** Usamos el descenso de gradiente para ajustar los pesos.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusi贸n**\n",
    " **Backpropagation** permite que las redes neuronales aprendan ajustando los pesos para minimizar el error.  \n",
    " **El Descenso de Gradiente** es el m茅todo clave para actualizar los pesos en la direcci贸n correcta.  \n",
    " **El uso de derivadas parciales** y la regla de la cadena son esenciales para calcular los ajustes.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

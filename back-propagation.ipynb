{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Backpropagation y Algoritmo de Descenso de Gradiente**\n",
    "\n",
    "## **¿Qué es el Backpropagation?**\n",
    "El **Backpropagation** (retropropagación) es un algoritmo de optimización utilizado en redes neuronales para **ajustar los pesos** y minimizar el error de predicción. Funciona propagando el error desde la capa de salida hacia las capas anteriores, ajustando los pesos mediante **descenso del gradiente**.\n",
    "\n",
    "Se basa en dos fases:\n",
    "1. **Forward Pass (Propagación hacia adelante):** La red genera una predicción.\n",
    "2. **Backward Pass (Retropropagación del error):** Calculamos el error y ajustamos los pesos.\n",
    "\n",
    "---\n",
    "\n",
    "## **¿Qué es el Descenso de Gradiente?**\n",
    "El **Descenso de Gradiente** es un algoritmo que encuentra los valores óptimos de los pesos minimizando una función de costo $( J(W))$. Se basa en calcular la derivada (gradiente) de la función de costo con respecto a los pesos y actualizar los valores en la dirección opuesta al gradiente.\n",
    "\n",
    "### **Fórmula general del Descenso de Gradiente**\n",
    "$$\n",
    "W := W - \\alpha \\frac{\\partial J}{\\partial W}\n",
    "$$\n",
    "Donde:\n",
    "- $ ( W ) $ son los pesos del modelo.\n",
    "- $ ( \\alpha )$ es la tasa de aprendizaje (learning rate).\n",
    "- $ ( \\frac{\\partial J}{\\partial W} ) $ es el gradiente de la función de costo respecto a $( W )$ .\n",
    "\n",
    "---\n",
    "\n",
    "## **Matemática del Backpropagation**\n",
    "### **Paso 1: Forward Pass**\n",
    "Dado un conjunto de datos de entrada $( X )$ y pesos $( W )$, la salida de una neurona en la capa oculta es:\n",
    "\n",
    "$$\n",
    "Z = W X + b\n",
    "$$\n",
    "\n",
    "Aplicamos la **función de activación** $( f(Z) )$ para obtener la salida de la neurona:\n",
    "\n",
    "$$\n",
    "A = f(Z)\n",
    "$$\n",
    "\n",
    "En la capa de salida, se obtiene una predicción $( \\hat{y} )$:\n",
    "\n",
    "$$\n",
    "\\hat{y} = f(W^{(2)} A + b^{(2)})\n",
    "$$\n",
    "\n",
    "### **Paso 2: Cálculo del Error**\n",
    "Definimos una función de pérdida $( J(W) )$, por ejemplo, el error cuadrático medio (MSE) en regresión:\n",
    "\n",
    "$$\n",
    "J(W) = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i)^2\n",
    "$$\n",
    "\n",
    "o la entropía cruzada en clasificación:\n",
    "\n",
    "$$\n",
    "J(W) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "### **Paso 3: Backpropagation - Cálculo del Gradiente**\n",
    "Usamos la **regla de la cadena** para calcular la derivada de la pérdida respecto a cada peso:\n",
    "\n",
    "1. **Error en la salida:**\n",
    "   $$\n",
    "   \\delta^{(2)} = \\frac{\\partial J}{\\partial \\hat{y}} \\cdot f'(Z^{(2)})\n",
    "   $$\n",
    "\n",
    "2. **Error en la capa oculta:**\n",
    "   $$\n",
    "   \\delta^{(1)} = (\\delta^{(2)} W^{(2)}) \\cdot f'(Z^{(1)})\n",
    "   $$\n",
    "\n",
    "3. **Gradiente de los pesos:**\n",
    "   $$\n",
    "   \\frac{\\partial J}{\\partial W^{(2)}} = A^{(1)} \\delta^{(2)}\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial J}{\\partial W^{(1)}} = X \\delta^{(1)}\n",
    "   $$\n",
    "\n",
    "### **Paso 4: Actualización de Pesos**\n",
    "$$\n",
    "W^{(l)} := W^{(l)} - \\alpha \\frac{\\partial J}{\\partial W^{(l)}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **Resumen del Algoritmo de Backpropagation**\n",
    "1. **Propagación hacia adelante:** Calculamos la salida de la red.\n",
    "2. **Cálculo del error:** Evaluamos la diferencia entre la salida esperada y la salida real.\n",
    "3. **Retropropagación del error:** Calculamos el gradiente en cada capa usando la regla de la cadena.\n",
    "4. **Actualización de pesos:** Usamos el descenso de gradiente para ajustar los pesos.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusión**\n",
    "📌 **Backpropagation** permite que las redes neuronales aprendan ajustando los pesos para minimizar el error.  \n",
    "📌 **El Descenso de Gradiente** es el método clave para actualizar los pesos en la dirección correcta.  \n",
    "📌 **El uso de derivadas parciales** y la regla de la cadena son esenciales para calcular los ajustes.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
